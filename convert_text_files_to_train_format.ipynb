{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.path.curdir, \"data\")\n",
    "SAVE_PATH = os.path.join(os.path.curdir, \"data\", \"training_formatted_data\")\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text_files_in_dir(path):\n",
    "    text_files = []\n",
    "    sub_directories = []\n",
    "    for item_name in os.listdir(DATA_PATH):\n",
    "        item_path = os.path.join(DATA_PATH, item_name)\n",
    "        if item_name.endswith(\".txt\") and os.path.isfile(item_path):\n",
    "            text_files.append(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            sub_directories.append(item_path)\n",
    "\n",
    "    for sub_directory in sub_directories:\n",
    "        for item_name in os.listdir(sub_directory):\n",
    "            item_path = os.path.join(sub_directory, item_name)\n",
    "            if item_name.endswith(\".txt\") and os.path.isfile(item_path):\n",
    "                text_files.append(item_path)\n",
    "\n",
    "    return text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_paths = find_text_files_in_dir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/saints_chapters/7_Keep_Up_Good_Courage.txt\n",
      "176\n"
     ]
    }
   ],
   "source": [
    "print(text_file_paths[0])\n",
    "print(len(text_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    }
   ],
   "source": [
    "for text_file_path in text_file_paths:\n",
    "    with open(text_file_path, \"r\") as text_file:\n",
    "        text = text_file.read()\n",
    "        formatted_data.append({\"text\": text})\n",
    "\n",
    "print(len(formatted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "# FORMAT SCRAPED DATA\n",
    "DC_DATA = os.path.join(DATA_PATH, \"dc\", \"scrape_dc.json\")\n",
    "TG_DATA = os.path.join(DATA_PATH, \"topical_guide\", \"scrape_tg.json\")\n",
    "CONF_DATA = os.path.join(DATA_PATH, \"conference_data\", \"conference_talks.json\")\n",
    "\n",
    "def format_json_entry(entry):\n",
    "    cleaned_text = \"\"\n",
    "\n",
    "    for key, value in entry.items():\n",
    "        cleaned_text += key + \":\\n\"\n",
    "        cleaned_text += value + \"\\n\"\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "with open(DC_DATA, \"r\") as dc_file:\n",
    "    dc_data = json.load(dc_file)\n",
    "    for entry in dc_data.values():\n",
    "        del entry[\"description\"]\n",
    "        formatted_data.append(format_json_entry(entry))\n",
    "\n",
    "with open(CONF_DATA, \"r\") as conf_file:\n",
    "    conf_data = json.load(conf_file)\n",
    "    for entry in conf_data.values():\n",
    "        formatted_data.append(format_json_entry(entry))\n",
    "\n",
    "print(len(formatted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = os.path.join(SAVE_PATH, \"train.json\")\n",
    "VALIDATION_SET_PATH = os.path.join(SAVE_PATH, \"validation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the data\n",
    "import random\n",
    "\n",
    "random.shuffle(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation sets\n",
    "train_validation_split_index = int(len(formatted_data) * 0.95)\n",
    "train_set = formatted_data[: train_validation_split_index]\n",
    "print(len(train_set))\n",
    "validation_set = formatted_data[train_validation_split_index:]\n",
    "print(len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TRAIN_SET_PATH), \"w+\") as train_file:\n",
    "    json.dump(train_set, train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(VALIDATION_SET_PATH), \"w+\") as validation_file:\n",
    "    json.dump(validation_set, validation_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "restoration_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
